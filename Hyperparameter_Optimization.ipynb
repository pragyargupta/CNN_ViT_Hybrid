{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce64ab9-093b-46ed-8672-a6225ef4c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Dataset Derm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fcc2a7-d2b3-4384-babd-421aa8c555fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model: swin_tiny\n",
      "✅ Best Feature Selection: Boruta\n",
      "✅ Best Classifier: SVM\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load tuning results\n",
    "df_results = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Cross_Dataset/hyperparameter_tuning_results.csv\")\n",
    "\n",
    "# Find the best model (highest F1-score or Accuracy)\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "\n",
    "# Extract best model, feature selection method, and classifier\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "\n",
    "print(f\"✅ Best Model: {best_model}\")\n",
    "print(f\"✅ Best Feature Selection: {best_feature_selection}\")\n",
    "print(f\"✅ Best Classifier: {best_classifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f91257-4911-4172-96ec-c80fe2b1bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Best Hyperparameter Tuning Result:\n",
      "Model                                                  swin_tiny\n",
      "Feature_Selection                                         Boruta\n",
      "Classifier                                                   SVM\n",
      "Best_Params          {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Accuracy                                                0.779783\n",
      "Precision                                               0.784818\n",
      "Recall                                                  0.773828\n",
      "F1-Score                                                0.778329\n",
      "Name: 40, dtype: object\n",
      "\n",
      "🔁 Evaluating with seed 42...\n",
      "\n",
      "🔁 Evaluating with seed 99...\n",
      "\n",
      "🔁 Evaluating with seed 123...\n",
      "\n",
      "📊 Final Evaluation (Different Seeds):\n",
      "Accuracy:  0.7798 ± 0.0000\n",
      "Precision: 0.7848 ± 0.0000\n",
      "Recall:    0.7738 ± 0.0000\n",
      "F1-Score:  0.7783 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Path to dataset directory\n",
    "BASE_DIR = \"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Cross_Dataset\"\n",
    "\n",
    "# ✅ Load best model info\n",
    "df_results = pd.read_csv(os.path.join(BASE_DIR, \"hyperparameter_tuning_results.csv\"))\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "best_params = eval(best_row[\"Best_Params\"])  # Convert string to dict\n",
    "# ✅ Print the best row\n",
    "print(\"\\n🏆 Best Hyperparameter Tuning Result:\")\n",
    "print(best_row)\n",
    "\n",
    "# ✅ Load features and labels\n",
    "X_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_features_{best_feature_selection}.npy\"))\n",
    "X_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_features_{best_feature_selection}.npy\"))\n",
    "y_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_labels.npy\")).astype(int)\n",
    "y_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_labels.npy\")).astype(int)\n",
    "\n",
    "# ✅ Seeds for evaluation\n",
    "seeds = [42, 99, 123]\n",
    "\n",
    "# ✅ Store metrics\n",
    "acc_list, prec_list, rec_list, f1_list = [], [], [], []\n",
    "\n",
    "# ✅ Run evaluation on different seeds\n",
    "for seed in seeds:\n",
    "    print(f\"\\n🔁 Evaluating with seed {seed}...\")\n",
    "\n",
    "    # Set classifier with current seed and best params\n",
    "    if best_classifier == \"SVM\":\n",
    "        clf = SVC(probability=True, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(random_state=seed, **best_params)\n",
    "    elif best_classifier == \"LogisticRegression\":\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"KNN\":\n",
    "        clf = KNeighborsClassifier(**best_params)\n",
    "    elif best_classifier == \"MLP\":\n",
    "        clf = MLPClassifier(random_state=seed, **best_params)\n",
    "\n",
    "    # Fit and evaluate\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc_list.append(accuracy_score(y_test, y_pred))\n",
    "    prec_list.append(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "    rec_list.append(recall_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_list.append(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "# ✅ Final summary\n",
    "print(\"\\n📊 Final Evaluation (Different Seeds):\")\n",
    "print(f\"Accuracy:  {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"Precision: {np.mean(prec_list):.4f} ± {np.std(prec_list):.4f}\")\n",
    "print(f\"Recall:    {np.mean(rec_list):.4f} ± {np.std(rec_list):.4f}\")\n",
    "print(f\"F1-Score:  {np.mean(f1_list):.4f} ± {np.std(f1_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e448f0-fe92-4af5-a2b7-738a678676a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Dataset SD260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfdd9b4-5b8e-4287-931d-a712f011f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model: convnext_tiny\n",
      "✅ Best Feature Selection: Kruskal-Wallis\n",
      "✅ Best Classifier: SVM\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load tuning results\n",
    "df_results = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Cross_Dataset/hyperparameter_tuning_results.csv\")\n",
    "\n",
    "# Find the best model (highest F1-score or Accuracy)\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "\n",
    "# Extract best model, feature selection method, and classifier\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "\n",
    "print(f\"✅ Best Model: {best_model}\")\n",
    "print(f\"✅ Best Feature Selection: {best_feature_selection}\")\n",
    "print(f\"✅ Best Classifier: {best_classifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4496f088-4285-4f71-8fff-2ae8b8fbe13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Best Hyperparameter Tuning Result:\n",
      "Model                                            convnext_tiny\n",
      "Feature_Selection                               Kruskal-Wallis\n",
      "Classifier                                                 SVM\n",
      "Best_Params          {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Accuracy                                              0.794203\n",
      "Precision                                             0.759104\n",
      "Recall                                                0.755411\n",
      "F1-Score                                              0.757183\n",
      "Name: 25, dtype: object\n",
      "\n",
      "🔁 Evaluating with seed 42...\n",
      "\n",
      "🔁 Evaluating with seed 99...\n",
      "\n",
      "🔁 Evaluating with seed 123...\n",
      "\n",
      "📊 Final Evaluation (Different Seeds):\n",
      "Accuracy:  0.7942 ± 0.0000\n",
      "Precision: 0.7591 ± 0.0000\n",
      "Recall:    0.7554 ± 0.0000\n",
      "F1-Score:  0.7572 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Path to dataset directory\n",
    "BASE_DIR = \"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Cross_Dataset\"\n",
    "\n",
    "# ✅ Load best model info\n",
    "df_results = pd.read_csv(os.path.join(BASE_DIR, \"hyperparameter_tuning_results.csv\"))\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "best_params = eval(best_row[\"Best_Params\"])  # Convert string to dict\n",
    "# ✅ Print the best row\n",
    "print(\"\\n🏆 Best Hyperparameter Tuning Result:\")\n",
    "print(best_row)\n",
    "\n",
    "# ✅ Load features and labels\n",
    "X_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_features_{best_feature_selection}.npy\"))\n",
    "X_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_features_{best_feature_selection}.npy\"))\n",
    "y_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_labels.npy\")).astype(int)\n",
    "y_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_labels.npy\")).astype(int)\n",
    "\n",
    "# ✅ Seeds for evaluation\n",
    "seeds = [42, 99, 123]\n",
    "\n",
    "# ✅ Store metrics\n",
    "acc_list, prec_list, rec_list, f1_list = [], [], [], []\n",
    "\n",
    "# ✅ Run evaluation on different seeds\n",
    "for seed in seeds:\n",
    "    print(f\"\\n🔁 Evaluating with seed {seed}...\")\n",
    "\n",
    "    # Set classifier with current seed and best params\n",
    "    if best_classifier == \"SVM\":\n",
    "        clf = SVC(probability=True, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(random_state=seed, **best_params)\n",
    "    elif best_classifier == \"LogisticRegression\":\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"KNN\":\n",
    "        clf = KNeighborsClassifier(**best_params)\n",
    "    elif best_classifier == \"MLP\":\n",
    "        clf = MLPClassifier(random_state=seed, **best_params)\n",
    "\n",
    "    # Fit and evaluate\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc_list.append(accuracy_score(y_test, y_pred))\n",
    "    prec_list.append(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "    rec_list.append(recall_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_list.append(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "# ✅ Final summary\n",
    "print(\"\\n📊 Final Evaluation (Different Seeds):\")\n",
    "print(f\"Accuracy:  {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"Precision: {np.mean(prec_list):.4f} ± {np.std(prec_list):.4f}\")\n",
    "print(f\"Recall:    {np.mean(rec_list):.4f} ± {np.std(rec_list):.4f}\")\n",
    "print(f\"F1-Score:  {np.mean(f1_list):.4f} ± {np.std(f1_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308fe2e-5d02-4307-8e8d-428c322bb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ssme Dataset Derm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408dda5b-a3af-4b2f-9866-0f1c57e69644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model: swin_tiny\n",
      "✅ Best Feature Selection: Boruta\n",
      "✅ Best Classifier: KNN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load tuning results\n",
    "df_results = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Same_Dataset/hyperparameter_tuning_results.csv\")\n",
    "\n",
    "# Find the best model (highest F1-score or Accuracy)\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "\n",
    "# Extract best model, feature selection method, and classifier\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "\n",
    "print(f\"✅ Best Model: {best_model}\")\n",
    "print(f\"✅ Best Feature Selection: {best_feature_selection}\")\n",
    "print(f\"✅ Best Classifier: {best_classifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5de5334-75cc-4ada-b8b3-ff7428e1eaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Best Hyperparameter Tuning Result:\n",
      "Model                                                swin_tiny\n",
      "Feature_Selection                                       Boruta\n",
      "Classifier                                                 KNN\n",
      "Best_Params          {'n_neighbors': 7, 'weights': 'distance'}\n",
      "Accuracy                                              0.837545\n",
      "Precision                                             0.864344\n",
      "Recall                                                0.830069\n",
      "F1-Score                                              0.842767\n",
      "Name: 43, dtype: object\n",
      "\n",
      "🔁 Evaluating with seed 42...\n",
      "\n",
      "🔁 Evaluating with seed 99...\n",
      "\n",
      "🔁 Evaluating with seed 123...\n",
      "\n",
      "📊 Final Evaluation (Different Seeds):\n",
      "Accuracy:  0.8412 ± 0.0000\n",
      "Precision: 0.8675 ± 0.0000\n",
      "Recall:    0.8332 ± 0.0000\n",
      "F1-Score:  0.8458 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Path to dataset directory\n",
    "BASE_DIR = \"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Same_Dataset\"\n",
    "\n",
    "# ✅ Load best model info\n",
    "df_results = pd.read_csv(os.path.join(BASE_DIR, \"hyperparameter_tuning_results.csv\"))\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "best_params = eval(best_row[\"Best_Params\"])  # Convert string to dict\n",
    "# ✅ Print the best row\n",
    "print(\"\\n🏆 Best Hyperparameter Tuning Result:\")\n",
    "print(best_row)\n",
    "\n",
    "# ✅ Load features and labels\n",
    "X_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_features_{best_feature_selection}.npy\"))\n",
    "X_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_features_{best_feature_selection}.npy\"))\n",
    "y_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_labels.npy\")).astype(int)\n",
    "y_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_labels.npy\")).astype(int)\n",
    "\n",
    "# ✅ Seeds for evaluation\n",
    "seeds = [42, 99, 123]\n",
    "\n",
    "# ✅ Store metrics\n",
    "acc_list, prec_list, rec_list, f1_list = [], [], [], []\n",
    "\n",
    "# ✅ Run evaluation on different seeds\n",
    "for seed in seeds:\n",
    "    print(f\"\\n🔁 Evaluating with seed {seed}...\")\n",
    "\n",
    "    # Set classifier with current seed and best params\n",
    "    if best_classifier == \"SVM\":\n",
    "        clf = SVC(probability=True, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(random_state=seed, **best_params)\n",
    "    elif best_classifier == \"LogisticRegression\":\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"KNN\":\n",
    "        clf = KNeighborsClassifier(**best_params)\n",
    "    elif best_classifier == \"MLP\":\n",
    "        clf = MLPClassifier(random_state=seed, **best_params)\n",
    "\n",
    "    # Fit and evaluate\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc_list.append(accuracy_score(y_test, y_pred))\n",
    "    prec_list.append(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "    rec_list.append(recall_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_list.append(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "# ✅ Final summary\n",
    "print(\"\\n📊 Final Evaluation (Different Seeds):\")\n",
    "print(f\"Accuracy:  {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"Precision: {np.mean(prec_list):.4f} ± {np.std(prec_list):.4f}\")\n",
    "print(f\"Recall:    {np.mean(rec_list):.4f} ± {np.std(rec_list):.4f}\")\n",
    "print(f\"F1-Score:  {np.mean(f1_list):.4f} ± {np.std(f1_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d6a31-a3e8-4829-8736-93f0375ad37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same Dataset SD260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bcf656-9d0d-48e5-8deb-02d5232488fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Model: swin_tiny\n",
      "✅ Best Feature Selection: Boruta\n",
      "✅ Best Classifier: KNN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load tuning results\n",
    "df_results = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Same_Dataset/hyperparameter_tuning_results.csv\")\n",
    "\n",
    "# Find the best model (highest F1-score or Accuracy)\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "\n",
    "# Extract best model, feature selection method, and classifier\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "\n",
    "print(f\"✅ Best Model: {best_model}\")\n",
    "print(f\"✅ Best Feature Selection: {best_feature_selection}\")\n",
    "print(f\"✅ Best Classifier: {best_classifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca83d92-35a0-430e-a173-dcfb572f2e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Best Hyperparameter Tuning Result:\n",
      "Model                                               swin_tiny\n",
      "Feature_Selection                                      Boruta\n",
      "Classifier                                                KNN\n",
      "Best_Params          {'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Accuracy                                             0.869565\n",
      "Precision                                            0.851678\n",
      "Recall                                               0.830043\n",
      "F1-Score                                             0.839279\n",
      "Name: 43, dtype: object\n",
      "\n",
      "🔁 Evaluating with seed 42...\n",
      "\n",
      "🔁 Evaluating with seed 99...\n",
      "\n",
      "🔁 Evaluating with seed 123...\n",
      "\n",
      "📊 Final Evaluation (Different Seeds):\n",
      "Accuracy:  0.8696 ± 0.0000\n",
      "Precision: 0.8517 ± 0.0000\n",
      "Recall:    0.8300 ± 0.0000\n",
      "F1-Score:  0.8393 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Path to dataset directory\n",
    "BASE_DIR = \"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Same_Dataset\"\n",
    "\n",
    "# ✅ Load best model info\n",
    "df_results = pd.read_csv(os.path.join(BASE_DIR, \"hyperparameter_tuning_results.csv\"))\n",
    "best_row = df_results.sort_values(by=\"Accuracy\", ascending=False).iloc[0]\n",
    "best_model = best_row[\"Model\"]\n",
    "best_feature_selection = best_row[\"Feature_Selection\"]\n",
    "best_classifier = best_row[\"Classifier\"]\n",
    "best_params = eval(best_row[\"Best_Params\"])  # Convert string to dict\n",
    "# ✅ Print the best row\n",
    "print(\"\\n🏆 Best Hyperparameter Tuning Result:\")\n",
    "print(best_row)\n",
    "\n",
    "# ✅ Load features and labels\n",
    "X_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_features_{best_feature_selection}.npy\"))\n",
    "X_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_features_{best_feature_selection}.npy\"))\n",
    "y_train = np.load(os.path.join(BASE_DIR, f\"{best_model}_train_labels.npy\")).astype(int)\n",
    "y_test = np.load(os.path.join(BASE_DIR, f\"{best_model}_test_labels.npy\")).astype(int)\n",
    "\n",
    "# ✅ Seeds for evaluation\n",
    "seeds = [42, 99, 123]\n",
    "\n",
    "# ✅ Store metrics\n",
    "acc_list, prec_list, rec_list, f1_list = [], [], [], []\n",
    "\n",
    "# ✅ Run evaluation on different seeds\n",
    "for seed in seeds:\n",
    "    print(f\"\\n🔁 Evaluating with seed {seed}...\")\n",
    "\n",
    "    # Set classifier with current seed and best params\n",
    "    if best_classifier == \"SVM\":\n",
    "        clf = SVC(probability=True, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(random_state=seed, **best_params)\n",
    "    elif best_classifier == \"LogisticRegression\":\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=seed, **best_params)\n",
    "    elif best_classifier == \"KNN\":\n",
    "        clf = KNeighborsClassifier(**best_params)\n",
    "    elif best_classifier == \"MLP\":\n",
    "        clf = MLPClassifier(random_state=seed, **best_params)\n",
    "\n",
    "    # Fit and evaluate\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc_list.append(accuracy_score(y_test, y_pred))\n",
    "    prec_list.append(precision_score(y_test, y_pred, average=\"macro\"))\n",
    "    rec_list.append(recall_score(y_test, y_pred, average=\"macro\"))\n",
    "    f1_list.append(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "# ✅ Final summary\n",
    "print(\"\\n📊 Final Evaluation (Different Seeds):\")\n",
    "print(f\"Accuracy:  {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"Precision: {np.mean(prec_list):.4f} ± {np.std(prec_list):.4f}\")\n",
    "print(f\"Recall:    {np.mean(rec_list):.4f} ± {np.std(rec_list):.4f}\")\n",
    "print(f\"F1-Score:  {np.mean(f1_list):.4f} ± {np.std(f1_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3471175-6e3a-495d-9862-5566ffae2c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
