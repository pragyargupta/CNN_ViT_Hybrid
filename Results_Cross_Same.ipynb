{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c6f9c0f-bd20-402c-aa9e-e3de5f6da4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN SET\n",
      "Eczema: 840 images\n",
      "Psoriasis: 854 images\n",
      "Tinea_Ringworm: 504 images\n",
      "Total in train: 2198 images\n",
      "\n",
      "VAL SET\n",
      "Eczema: 105 images\n",
      "Psoriasis: 107 images\n",
      "Tinea_Ringworm: 63 images\n",
      "Total in val: 275 images\n",
      "\n",
      "TEST SET\n",
      "Eczema: 106 images\n",
      "Psoriasis: 107 images\n",
      "Tinea_Ringworm: 64 images\n",
      "Total in test: 277 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths\n",
    "dataset_dir = \"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset\"\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "# Count images\n",
    "for split in splits:\n",
    "    print(f\"\\n{split.upper()} SET\")\n",
    "    split_path = os.path.join(dataset_dir, split)\n",
    "    total = 0\n",
    "    for class_name in os.listdir(split_path):\n",
    "        class_path = os.path.join(split_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            num_images = len([file for file in os.listdir(class_path) if file.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            total += num_images\n",
    "            print(f\"{class_name}: {num_images} images\")\n",
    "    print(f\"Total in {split}: {total} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af125b2-c6e3-4234-9185-22dd295d4a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN SET\n",
      "Eczema: 1112 images\n",
      "Psoriasis: 445 images\n",
      "Tinea_Ringworm: 1201 images\n",
      "Total in train: 2758 images\n",
      "\n",
      "VAL SET\n",
      "Eczema: 139 images\n",
      "Psoriasis: 56 images\n",
      "Tinea_Ringworm: 150 images\n",
      "Total in val: 345 images\n",
      "\n",
      "TEST SET\n",
      "Eczema: 139 images\n",
      "Psoriasis: 56 images\n",
      "Tinea_Ringworm: 150 images\n",
      "Total in test: 345 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths\n",
    "dataset_dir = \"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset\"\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "# Count images\n",
    "for split in splits:\n",
    "    print(f\"\\n{split.upper()} SET\")\n",
    "    split_path = os.path.join(dataset_dir, split)\n",
    "    total = 0\n",
    "    for class_name in os.listdir(split_path):\n",
    "        class_path = os.path.join(split_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            num_images = len([file for file in os.listdir(class_path) if file.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            total += num_images\n",
    "            print(f\"{class_name}: {num_images} images\")\n",
    "    print(f\"Total in {split}: {total} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33286dc-b4c9-499c-b3e1-dfd9ed7c20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derm Cross Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db690176-84cc-404f-9ec4-825a4f535f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Cross_Dataset/multi_model_runs_cross_dataset/all_cross_dataset_model_runs_metrics_derm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ddf6f3c-2698-4a46-91ff-84ee150cbefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model Feature Selection          Classifier  Accuracy_mean  \\\n",
      "0  convnext_tiny            Boruta              1D CNN       0.409146   \n",
      "1  convnext_tiny            Boruta                 KNN       0.364621   \n",
      "2  convnext_tiny            Boruta  LogisticRegression       0.393502   \n",
      "3  convnext_tiny            Boruta                 MLP       0.381468   \n",
      "4  convnext_tiny            Boruta        RandomForest       0.405535   \n",
      "\n",
      "   Accuracy_std  Precision_mean  Precision_std  Recall_mean  Recall_std  \\\n",
      "0      0.026611        0.315275       0.036604     0.409146    0.026611   \n",
      "1      0.003610        0.326206       0.002999     0.364621    0.003610   \n",
      "2      0.000000        0.376241       0.000000     0.393502    0.000000   \n",
      "3      0.024037        0.378266       0.020503     0.381468    0.024037   \n",
      "4      0.016674        0.366316       0.017056     0.405535    0.016674   \n",
      "\n",
      "   F1-Score_mean  ...  AUC_mean   AUC_std  Inference Time (s)_mean  \\\n",
      "0       0.321081  ...  0.496561  0.021145                 0.001226   \n",
      "1       0.334351  ...  0.506030  0.002542                 0.049759   \n",
      "2       0.382258  ...  0.523916  0.000128                 0.000278   \n",
      "3       0.378942  ...  0.526802  0.010410                 0.000294   \n",
      "4       0.369992  ...  0.537466  0.011785                 0.005366   \n",
      "\n",
      "   Inference Time (s)_std  Accuracy_formatted Precision_formatted  \\\n",
      "0                0.001119        40.9% ± 2.7%        31.5% ± 3.7%   \n",
      "1                0.037340        36.5% ± 0.4%        32.6% ± 0.3%   \n",
      "2                0.000006        39.4% ± 0.0%        37.6% ± 0.0%   \n",
      "3                0.000028        38.1% ± 2.4%        37.8% ± 2.1%   \n",
      "4                0.000843        40.6% ± 1.7%        36.6% ± 1.7%   \n",
      "\n",
      "  Recall_formatted F1-Score_formatted AUC_formatted  \\\n",
      "0     40.9% ± 2.7%       32.1% ± 5.5%  49.7% ± 2.1%   \n",
      "1     36.5% ± 0.4%       33.4% ± 0.3%  50.6% ± 0.3%   \n",
      "2     39.4% ± 0.0%       38.2% ± 0.0%  52.4% ± 0.0%   \n",
      "3     38.1% ± 2.4%       37.9% ± 2.2%  52.7% ± 1.0%   \n",
      "4     40.6% ± 1.7%       37.0% ± 1.3%  53.7% ± 1.2%   \n",
      "\n",
      "  Inference Time (s)_formatted  \n",
      "0              1.2 ms ± 1.1 ms  \n",
      "1            49.8 ms ± 37.3 ms  \n",
      "2              0.3 ms ± 0.0 ms  \n",
      "3              0.3 ms ± 0.0 ms  \n",
      "4              5.4 ms ± 0.8 ms  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of metric columns\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC','Inference Time (s)']\n",
    "\n",
    "# Group by Model, Feature Selection method, and Classifier\n",
    "summary = df.groupby(['Model', 'Feature Selection', 'Classifier'])[metrics].agg(['mean', 'std'])\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "\n",
    "# Reset index to get back columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Format metric columns\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    summary[f'{metric}_formatted'] = summary.apply(\n",
    "        lambda row: f\"{row[f'{metric}_mean']*100:.1f}% ± {row[f'{metric}_std']*100:.1f}%\", axis=1\n",
    "    )\n",
    "\n",
    "# Format Inference Time separately (in milliseconds)\n",
    "summary['Inference Time (s)_formatted'] = summary.apply(\n",
    "    lambda row: f\"{row['Inference Time (s)_mean']*1000:.1f} ms ± {row['Inference Time (s)_std']*1000:.1f} ms\", axis=1\n",
    ")\n",
    "plus_minus = '\\u00b1'  # This is the ± symbol\n",
    "# Show or export\n",
    "print(summary.head())  \n",
    "summary.to_excel(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Cross_Dataset/multi_model_runs_cross_dataset/summary_metrics_cross_derm.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93896a-7837-4bdc-85c2-84487fac5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derm Same Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49d9d846-982e-4135-b4ea-41784434d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Same_Dataset/multi_model_runs_same_dataset/all_same_dataset_model_runs_metrics_derm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df97f271-a816-49d1-912c-03cd9f4defaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model Feature Selection          Classifier  Accuracy_mean  \\\n",
      "0  convnext_tiny            Boruta              1D CNN       0.809868   \n",
      "1  convnext_tiny            Boruta                 KNN       0.819495   \n",
      "2  convnext_tiny            Boruta  LogisticRegression       0.823105   \n",
      "3  convnext_tiny            Boruta                 MLP       0.817088   \n",
      "4  convnext_tiny            Boruta        RandomForest       0.808664   \n",
      "\n",
      "   Accuracy_std  Precision_mean  Precision_std  Recall_mean  Recall_std  \\\n",
      "0      0.005515        0.811201       0.004875     0.809868    0.005515   \n",
      "1      0.000000        0.822493       0.000000     0.819495    0.000000   \n",
      "2      0.000000        0.824772       0.000000     0.823105    0.000000   \n",
      "3      0.004169        0.818174       0.003559     0.817088    0.004169   \n",
      "4      0.007220        0.810106       0.006387     0.808664    0.007220   \n",
      "\n",
      "   F1-Score_mean  ...  AUC_mean   AUC_std  Inference Time (s)_mean  \\\n",
      "0       0.809813  ...  0.930232  0.001403                 0.001040   \n",
      "1       0.819153  ...  0.893618  0.000000                 0.009501   \n",
      "2       0.822884  ...  0.938976  0.000000                 0.000330   \n",
      "3       0.816650  ...  0.940032  0.000268                 0.000431   \n",
      "4       0.808224  ...  0.932113  0.003846                 0.002398   \n",
      "\n",
      "   Inference Time (s)_std  Accuracy_formatted Precision_formatted  \\\n",
      "0                0.000963        81.0% ± 0.6%        81.1% ± 0.5%   \n",
      "1                0.002663        81.9% ± 0.0%        82.2% ± 0.0%   \n",
      "2                0.000043        82.3% ± 0.0%        82.5% ± 0.0%   \n",
      "3                0.000204        81.7% ± 0.4%        81.8% ± 0.4%   \n",
      "4                0.000045        80.9% ± 0.7%        81.0% ± 0.6%   \n",
      "\n",
      "  Recall_formatted F1-Score_formatted AUC_formatted  \\\n",
      "0     81.0% ± 0.6%       81.0% ± 0.6%  93.0% ± 0.1%   \n",
      "1     81.9% ± 0.0%       81.9% ± 0.0%  89.4% ± 0.0%   \n",
      "2     82.3% ± 0.0%       82.3% ± 0.0%  93.9% ± 0.0%   \n",
      "3     81.7% ± 0.4%       81.7% ± 0.4%  94.0% ± 0.0%   \n",
      "4     80.9% ± 0.7%       80.8% ± 0.7%  93.2% ± 0.4%   \n",
      "\n",
      "  Inference Time (s)_formatted  \n",
      "0              1.0 ms ± 1.0 ms  \n",
      "1              9.5 ms ± 2.7 ms  \n",
      "2              0.3 ms ± 0.0 ms  \n",
      "3              0.4 ms ± 0.2 ms  \n",
      "4              2.4 ms ± 0.0 ms  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of metric columns\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC','Inference Time (s)']\n",
    "\n",
    "# Group by Model, Feature Selection method, and Classifier\n",
    "summary = df.groupby(['Model', 'Feature Selection', 'Classifier'])[metrics].agg(['mean', 'std'])\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "\n",
    "# Reset index to get back columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Format metric columns\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    summary[f'{metric}_formatted'] = summary.apply(\n",
    "        lambda row: f\"{row[f'{metric}_mean']*100:.1f}% ± {row[f'{metric}_std']*100:.1f}%\", axis=1\n",
    "    )\n",
    "\n",
    "# Format Inference Time separately (in milliseconds)\n",
    "summary['Inference Time (s)_formatted'] = summary.apply(\n",
    "    lambda row: f\"{row['Inference Time (s)_mean']*1000:.1f} ms ± {row['Inference Time (s)_std']*1000:.1f} ms\", axis=1\n",
    ")\n",
    "plus_minus = '\\u00b1'  # This is the ± symbol\n",
    "# Show or export\n",
    "print(summary.head())  \n",
    "summary.to_excel(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Same_Dataset/multi_model_runs_same_dataset/summary_metrics_same_derm.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ba788-0193-4d2e-8745-5e7ddc8749dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD260 Cross Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814fd168-4758-4e43-ba5e-3335ae9b4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Cross_Dataset/multi_model_runs_cross_dataset/all_cross_dataset_model_runs_metrics_SD260.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e45a6c65-003a-495e-a2a8-1f41feefb09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model Feature Selection          Classifier  Accuracy_mean  \\\n",
      "0  convnext_tiny            Boruta              1D CNN       0.713043   \n",
      "1  convnext_tiny            Boruta                 KNN       0.718841   \n",
      "2  convnext_tiny            Boruta  LogisticRegression       0.726570   \n",
      "3  convnext_tiny            Boruta                 MLP       0.751691   \n",
      "4  convnext_tiny            Boruta        RandomForest       0.716908   \n",
      "\n",
      "   Accuracy_std  Precision_mean  Precision_std  Recall_mean  Recall_std  \\\n",
      "0      0.027650        0.721819       0.017342     0.713043    0.027650   \n",
      "1      0.000000        0.711742       0.000000     0.718841    0.000000   \n",
      "2      0.001673        0.722718       0.002087     0.726570    0.001673   \n",
      "3      0.017710        0.749145       0.018880     0.751691    0.017710   \n",
      "4      0.011714        0.714793       0.011916     0.716908    0.011714   \n",
      "\n",
      "   F1-Score_mean  ...  AUC_mean   AUC_std  Inference Time (s)_mean  \\\n",
      "0       0.702350  ...  0.877669  0.000751                 0.001162   \n",
      "1       0.709613  ...  0.839466  0.000000                 0.008233   \n",
      "2       0.724227  ...  0.878610  0.000042                 0.000343   \n",
      "3       0.749927  ...  0.898452  0.004078                 0.000366   \n",
      "4       0.709699  ...  0.867251  0.005155                 0.005092   \n",
      "\n",
      "   Inference Time (s)_std  Accuracy_formatted Precision_formatted  \\\n",
      "0                0.000971        71.3% ± 2.8%        72.2% ± 1.7%   \n",
      "1                0.000799        71.9% ± 0.0%        71.2% ± 0.0%   \n",
      "2                0.000037        72.7% ± 0.2%        72.3% ± 0.2%   \n",
      "3                0.000043        75.2% ± 1.8%        74.9% ± 1.9%   \n",
      "4                0.000121        71.7% ± 1.2%        71.5% ± 1.2%   \n",
      "\n",
      "  Recall_formatted F1-Score_formatted AUC_formatted  \\\n",
      "0     71.3% ± 2.8%       70.2% ± 3.3%  87.8% ± 0.1%   \n",
      "1     71.9% ± 0.0%       71.0% ± 0.0%  83.9% ± 0.0%   \n",
      "2     72.7% ± 0.2%       72.4% ± 0.2%  87.9% ± 0.0%   \n",
      "3     75.2% ± 1.8%       75.0% ± 1.8%  89.8% ± 0.4%   \n",
      "4     71.7% ± 1.2%       71.0% ± 1.2%  86.7% ± 0.5%   \n",
      "\n",
      "  Inference Time (s)_formatted  \n",
      "0              1.2 ms ± 1.0 ms  \n",
      "1              8.2 ms ± 0.8 ms  \n",
      "2              0.3 ms ± 0.0 ms  \n",
      "3              0.4 ms ± 0.0 ms  \n",
      "4              5.1 ms ± 0.1 ms  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of metric columns\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC','Inference Time (s)']\n",
    "\n",
    "# Group by Model, Feature Selection method, and Classifier\n",
    "summary = df.groupby(['Model', 'Feature Selection', 'Classifier'])[metrics].agg(['mean', 'std'])\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "\n",
    "# Reset index to get back columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Format metric columns\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    summary[f'{metric}_formatted'] = summary.apply(\n",
    "        lambda row: f\"{row[f'{metric}_mean']*100:.1f}% ± {row[f'{metric}_std']*100:.1f}%\", axis=1\n",
    "    )\n",
    "\n",
    "# Format Inference Time separately (in milliseconds)\n",
    "summary['Inference Time (s)_formatted'] = summary.apply(\n",
    "    lambda row: f\"{row['Inference Time (s)_mean']*1000:.1f} ms ± {row['Inference Time (s)_std']*1000:.1f} ms\", axis=1\n",
    ")\n",
    "plus_minus = '\\u00b1'  # This is the ± symbol\n",
    "# Show or export\n",
    "print(summary.head())  \n",
    "summary.to_excel(\"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Cross_Dataset/multi_model_runs_cross_dataset/summary_metrics_cross_SD260.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99bbd4-484e-4ba9-9ae6-215a70f579c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SD260 Same Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1391acdb-5fe3-4989-95cc-a5c855fbc833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Same_Dataset/multi_model_runs_same_dataset/all_same_dataset_model_runs_metrics_SD260.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ad21a4-d108-4c50-9021-5f68ba4d9abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model Feature Selection          Classifier  Accuracy_mean  \\\n",
      "0  convnext_tiny            Boruta              1D CNN       0.805797   \n",
      "1  convnext_tiny            Boruta                 KNN       0.831884   \n",
      "2  convnext_tiny            Boruta  LogisticRegression       0.823188   \n",
      "3  convnext_tiny            Boruta                 MLP       0.823188   \n",
      "4  convnext_tiny            Boruta        RandomForest       0.807729   \n",
      "\n",
      "   Accuracy_std  Precision_mean  Precision_std  Recall_mean  Recall_std  \\\n",
      "0      0.005020        0.805615       0.004299     0.805797    0.005020   \n",
      "1      0.000000        0.834607       0.000000     0.831884    0.000000   \n",
      "2      0.000000        0.826118       0.000000     0.823188    0.000000   \n",
      "3      0.002899        0.826289       0.002612     0.823188    0.002899   \n",
      "4      0.004428        0.807406       0.004325     0.807729    0.004428   \n",
      "\n",
      "   F1-Score_mean  ...  AUC_mean   AUC_std  Inference Time (s)_mean  \\\n",
      "0       0.803026  ...  0.920600  0.001641                 0.001130   \n",
      "1       0.828131  ...  0.881856  0.000000                 0.047904   \n",
      "2       0.819021  ...  0.927120  0.000000                 0.000366   \n",
      "3       0.819667  ...  0.925715  0.000387                 0.000374   \n",
      "4       0.805167  ...  0.920557  0.002135                 0.002538   \n",
      "\n",
      "   Inference Time (s)_std  Accuracy_formatted Precision_formatted  \\\n",
      "0                0.001020        80.6% ± 0.5%        80.6% ± 0.4%   \n",
      "1                0.035343        83.2% ± 0.0%        83.5% ± 0.0%   \n",
      "2                0.000043        82.3% ± 0.0%        82.6% ± 0.0%   \n",
      "3                0.000016        82.3% ± 0.3%        82.6% ± 0.3%   \n",
      "4                0.000047        80.8% ± 0.4%        80.7% ± 0.4%   \n",
      "\n",
      "  Recall_formatted F1-Score_formatted AUC_formatted  \\\n",
      "0     80.6% ± 0.5%       80.3% ± 0.7%  92.1% ± 0.2%   \n",
      "1     83.2% ± 0.0%       82.8% ± 0.0%  88.2% ± 0.0%   \n",
      "2     82.3% ± 0.0%       81.9% ± 0.0%  92.7% ± 0.0%   \n",
      "3     82.3% ± 0.3%       82.0% ± 0.3%  92.6% ± 0.0%   \n",
      "4     80.8% ± 0.4%       80.5% ± 0.4%  92.1% ± 0.2%   \n",
      "\n",
      "  Inference Time (s)_formatted  \n",
      "0              1.1 ms ± 1.0 ms  \n",
      "1            47.9 ms ± 35.3 ms  \n",
      "2              0.4 ms ± 0.0 ms  \n",
      "3              0.4 ms ± 0.0 ms  \n",
      "4              2.5 ms ± 0.0 ms  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of metric columns\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC','Inference Time (s)']\n",
    "\n",
    "# Group by Model, Feature Selection method, and Classifier\n",
    "summary = df.groupby(['Model', 'Feature Selection', 'Classifier'])[metrics].agg(['mean', 'std'])\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "\n",
    "# Reset index to get back columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Format metric columns\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    summary[f'{metric}_formatted'] = summary.apply(\n",
    "        lambda row: f\"{row[f'{metric}_mean']*100:.1f}% ± {row[f'{metric}_std']*100:.1f}%\", axis=1\n",
    "    )\n",
    "\n",
    "# Format Inference Time separately (in milliseconds)\n",
    "summary['Inference Time (s)_formatted'] = summary.apply(\n",
    "    lambda row: f\"{row['Inference Time (s)_mean']*1000:.1f} ms ± {row['Inference Time (s)_std']*1000:.1f} ms\", axis=1\n",
    ")\n",
    "plus_minus = '\\u00b1'  # This is the ± symbol\n",
    "# Show or export\n",
    "print(summary.head())  \n",
    "summary.to_excel(\"/mnt/c/Users/pragy/Downloads/Dataset/SD260/Split_Dataset/Features_Same_Dataset/multi_model_runs_same_dataset/summary_metrics_same_SD260.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5c5ca-04d9-4063-8961-598052c3b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same_Dataset_Derm_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc63217d-aae6-4923-826e-bd39d7678f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Same_Dataset/Layers/layerwise_model_results/all_same_dataset_model_runs_metrics_layers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11c3a2bd-549c-4095-a4b9-67bd51517d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Feature Selection   Layer          Classifier  Accuracy_mean  \\\n",
      "0  resnet50            Boruta  layer1                 KNN       0.631769   \n",
      "1  resnet50            Boruta  layer1  LogisticRegression       0.545126   \n",
      "2  resnet50            Boruta  layer1                 MLP       0.634176   \n",
      "3  resnet50            Boruta  layer1        RandomForest       0.660650   \n",
      "4  resnet50            Boruta  layer1                 SVM       0.617329   \n",
      "\n",
      "   Accuracy_std  Precision_mean  Precision_std  Recall_mean  Recall_std  ...  \\\n",
      "0      0.000000        0.671824       0.000000     0.603830    0.000000  ...   \n",
      "1      0.000000        0.577462       0.000000     0.510226    0.000000  ...   \n",
      "2      0.023210        0.655527       0.011824     0.621699    0.022492  ...   \n",
      "3      0.015736        0.691540       0.017386     0.639335    0.014076  ...   \n",
      "4      0.000000        0.676214       0.000000     0.587447    0.000000  ...   \n",
      "\n",
      "   AUC_mean   AUC_std  Inference Time (s)_mean  Inference Time (s)_std  \\\n",
      "0  0.789013  0.000000                 0.027183                0.019090   \n",
      "1  0.744678  0.000000                 0.000157                0.000057   \n",
      "2  0.815489  0.004616                 0.000594                0.000545   \n",
      "3  0.835770  0.003030                 0.005848                0.001212   \n",
      "4  0.793964  0.000273                 0.023858                0.001333   \n",
      "\n",
      "   Accuracy_formatted  Precision_formatted Recall_formatted  \\\n",
      "0        63.2% ± 0.0%         67.2% ± 0.0%     60.4% ± 0.0%   \n",
      "1        54.5% ± 0.0%         57.7% ± 0.0%     51.0% ± 0.0%   \n",
      "2        63.4% ± 2.3%         65.6% ± 1.2%     62.2% ± 2.2%   \n",
      "3        66.1% ± 1.6%         69.2% ± 1.7%     63.9% ± 1.4%   \n",
      "4        61.7% ± 0.0%         67.6% ± 0.0%     58.7% ± 0.0%   \n",
      "\n",
      "  F1-Score_formatted AUC_formatted Inference Time (s)_formatted  \n",
      "0       61.7% ± 0.0%  78.9% ± 0.0%            27.2 ms ± 19.1 ms  \n",
      "1       51.4% ± 0.0%  74.5% ± 0.0%              0.2 ms ± 0.1 ms  \n",
      "2       63.0% ± 2.2%  81.5% ± 0.5%              0.6 ms ± 0.5 ms  \n",
      "3       65.2% ± 1.3%  83.6% ± 0.3%              5.8 ms ± 1.2 ms  \n",
      "4       59.9% ± 0.0%  79.4% ± 0.0%             23.9 ms ± 1.3 ms  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of metric columns\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC','Inference Time (s)']\n",
    "\n",
    "# Group by Model, Feature Selection method, and Classifier\n",
    "summary = df.groupby(['Model', 'Feature Selection', 'Layer','Classifier'])[metrics].agg(['mean', 'std'])\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "\n",
    "# Reset index to get back columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Format metric columns\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    summary[f'{metric}_formatted'] = summary.apply(\n",
    "        lambda row: f\"{row[f'{metric}_mean']*100:.1f}% ± {row[f'{metric}_std']*100:.1f}%\", axis=1\n",
    "    )\n",
    "\n",
    "# Format Inference Time separately (in milliseconds)\n",
    "summary['Inference Time (s)_formatted'] = summary.apply(\n",
    "    lambda row: f\"{row['Inference Time (s)_mean']*1000:.1f} ms ± {row['Inference Time (s)_std']*1000:.1f} ms\", axis=1\n",
    ")\n",
    "plus_minus = '\\u00b1'  # This is the ± symbol\n",
    "# Show or export\n",
    "print(summary.head())  \n",
    "summary.to_excel(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Same_Dataset/Layers/layerwise_model_results/all_same_dataset_layers.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b8185-8f65-4477-b49e-be17eed594f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross_Dataset_Derm_Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58a557e-ac11-4dc8-8bc4-a5bf6e1b766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Cross_Dataset/Layers/layerwise_model_results/all_cross_dataset_model_runs_metrics_layers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3934bd0-1196-4f60-a6d0-54a897911579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Feature Selection   Layer          Classifier  Accuracy_mean  \\\n",
      "0  resnet50            Boruta  layer1                 KNN       0.657040   \n",
      "1  resnet50            Boruta  layer1  LogisticRegression       0.559567   \n",
      "2  resnet50            Boruta  layer1                 MLP       0.635379   \n",
      "3  resnet50            Boruta  layer1        RandomForest       0.701564   \n",
      "4  resnet50            Boruta  layer1                 SVM       0.595668   \n",
      "\n",
      "   Accuracy_std  Precision_mean  Precision_std  Recall_mean  Recall_std  ...  \\\n",
      "0      0.000000        0.674057       0.000000     0.625725    0.000000  ...   \n",
      "1      0.000000        0.593519       0.000000     0.516437    0.000000  ...   \n",
      "2      0.019103        0.650496       0.037209     0.623611    0.011734  ...   \n",
      "3      0.012678        0.734912       0.013715     0.678198    0.011984  ...   \n",
      "4      0.000000        0.654582       0.000000     0.562388    0.000000  ...   \n",
      "\n",
      "   AUC_mean   AUC_std  Inference Time (s)_mean  Inference Time (s)_std  \\\n",
      "0  0.778773  0.000000                 0.015816                0.001210   \n",
      "1  0.731684  0.000000                 0.000172                0.000079   \n",
      "2  0.821650  0.008917                 0.000274                0.000023   \n",
      "3  0.852528  0.003442                 0.005816                0.001247   \n",
      "4  0.785089  0.000171                 0.030202                0.003926   \n",
      "\n",
      "   Accuracy_formatted  Precision_formatted Recall_formatted  \\\n",
      "0        65.7% ± 0.0%         67.4% ± 0.0%     62.6% ± 0.0%   \n",
      "1        56.0% ± 0.0%         59.4% ± 0.0%     51.6% ± 0.0%   \n",
      "2        63.5% ± 1.9%         65.0% ± 3.7%     62.4% ± 1.2%   \n",
      "3        70.2% ± 1.3%         73.5% ± 1.4%     67.8% ± 1.2%   \n",
      "4        59.6% ± 0.0%         65.5% ± 0.0%     56.2% ± 0.0%   \n",
      "\n",
      "  F1-Score_formatted AUC_formatted Inference Time (s)_formatted  \n",
      "0       63.5% ± 0.0%  77.9% ± 0.0%             15.8 ms ± 1.2 ms  \n",
      "1       51.5% ± 0.0%  73.2% ± 0.0%              0.2 ms ± 0.1 ms  \n",
      "2       62.9% ± 1.7%  82.2% ± 0.9%              0.3 ms ± 0.0 ms  \n",
      "3       69.2% ± 1.2%  85.3% ± 0.3%              5.8 ms ± 1.2 ms  \n",
      "4       57.2% ± 0.0%  78.5% ± 0.0%             30.2 ms ± 3.9 ms  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of metric columns\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC','Inference Time (s)']\n",
    "\n",
    "# Group by Model, Feature Selection method, and Classifier\n",
    "summary = df.groupby(['Model', 'Feature Selection', 'Layer','Classifier'])[metrics].agg(['mean', 'std'])\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "\n",
    "# Reset index to get back columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Format metric columns\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']:\n",
    "    summary[f'{metric}_formatted'] = summary.apply(\n",
    "        lambda row: f\"{row[f'{metric}_mean']*100:.1f}% ± {row[f'{metric}_std']*100:.1f}%\", axis=1\n",
    "    )\n",
    "\n",
    "# Format Inference Time separately (in milliseconds)\n",
    "summary['Inference Time (s)_formatted'] = summary.apply(\n",
    "    lambda row: f\"{row['Inference Time (s)_mean']*1000:.1f} ms ± {row['Inference Time (s)_std']*1000:.1f} ms\", axis=1\n",
    ")\n",
    "plus_minus = '\\u00b1'  # This is the ± symbol\n",
    "# Show or export\n",
    "print(summary.head())  \n",
    "summary.to_excel(\"/mnt/c/Users/pragy/Downloads/Dataset/Original Images/Split_Dataset/Features_Cross_Dataset/Layers/layerwise_model_results/all_cross_dataset_layers.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e010861a-bc16-4c9f-9c3c-f0c84a73d8e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intra vs Cross (Feature ML): t = 13.098, p = 0.0002\n",
      "Intra vs Cross (End-to-End): t = 13.115, p = 0.0002\n",
      "Feature vs End-to-End (Intra): t = 1.367, p = 0.2433\n",
      "Feature vs End-to-End (Cross): t = 14.597, p = 0.0001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Example mock data — replace with your actual scores\n",
    "# Format: [run1, run2, run3, ...] for paired comparisons\n",
    "cnn_A_A_ml_A       = np.array([0.823, 0.819, 0.808, 0.809, 0.827])\n",
    "cnn_A_B_ml_B       = np.array([0.725, 0.687, 0.669, 0.693, 0.672])\n",
    "\n",
    "cnn_A_end_to_end_A = np.array([0.797, 0.761, 0.844, 0.772, 0.805])\n",
    "cnn_A_end_to_end_B = np.array([0.429, 0.458, 0.359, 0.362, 0.426])\n",
    "\n",
    "# 1. Intra vs Cross (Feature ML)\n",
    "t_stat_1, p_val_1 = ttest_rel(cnn_A_A_ml_A, cnn_A_B_ml_B)\n",
    "print(f\"Intra vs Cross (Feature ML): t = {t_stat_1:.3f}, p = {p_val_1:.4f}\")\n",
    "\n",
    "# 2. Intra vs Cross (End-to-End)\n",
    "t_stat_2, p_val_2 = ttest_rel(cnn_A_end_to_end_A, cnn_A_end_to_end_B)\n",
    "print(f\"Intra vs Cross (End-to-End): t = {t_stat_2:.3f}, p = {p_val_2:.4f}\")\n",
    "\n",
    "# 3. Feature vs End-to-End (Intra)\n",
    "t_stat_3, p_val_3 = ttest_rel(cnn_A_A_ml_A, cnn_A_end_to_end_A)\n",
    "print(f\"Feature vs End-to-End (Intra): t = {t_stat_3:.3f}, p = {p_val_3:.4f}\")\n",
    "\n",
    "# 4. Feature vs End-to-End (Cross)\n",
    "t_stat_4, p_val_4 = ttest_rel(cnn_A_B_ml_B, cnn_A_end_to_end_B)\n",
    "print(f\"Feature vs End-to-End (Cross): t = {t_stat_4:.3f}, p = {p_val_4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d737f23-7869-4748-a2a8-b71f9ca0ee8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
